---
title: Running pose estimation on the SWC HPC system
author: Adam Tyson, Niko Sirmpilatze & Laura Porta
execute: 
  enabled: true
format:
    revealjs:
        theme: [default, niu-light.scss]
        logo: img/logo_niu_light.png
        footer: "SWC | 2023-12-06"
        slide-number: c
        menu:
            numbers: true
        chalkboard: true
        scrollable: true
        preview-links: false
        view-distance: 10
        mobile-view-distance: 10
        auto-animate: true
        auto-play-media: true
        code-overflow: wrap
        highlight-style: atom-one
        mermaid: 
          theme: neutral
          fontFamily: arial
          curve: linear
    html:
        theme: [default, niu-light.scss]
        logo: img/logo_niu_light.png
        date: "2023-12-06"
        toc: true
        code-overflow: scroll
        highlight-style: atom-one
        mermaid: 
          theme: neutral
          fontFamily: arial
          curve: linear
          margin-left: 0
        embed-resources: true
        page-layout: full
my-custom-stuff:
   my-reuseable-variable: "I can use this wherever I want in the markdown, and change it in only once place :)"
---


## Contents

* Introduction to High Performance Computing
* SWC HPC system
* Using the job scheduler
* Running pose estimation on the SWC HPC

## Introduction to High Performance Computing (HPC) {.smaller}
* Lots of meanings
* Often just a system with many machines (nodes) linked together with some/all of:
  * Lots of CPU cores per node
  * Powerful GPUs
  * Lots of memory per node
  * Fast networking to link nodes
  * Fast data storage
  * Standardised software installation

## Why?
* Run jobs too large for desktop workstations
* Run many jobs at once
* Efficiency (cheaper to have central machines running 24/7)

. . .

* In neuroscience, typically used for:
  * Analysing large data (e.g. high memory requirements)
  * Parallelising analysis/modelling (run on many machines at once)


## SWC HPC hardware
(Correct at time of writing)

* Ubuntu 20.04
* 81 nodes
  * 46 CPU nodes
  * 35 GPU nodes
* 3000 CPU cores
* 83 GPUs
* ~20TB RAM

## Logging in

Log into bastion node (not necessary within SWC network)
```bash
ssh <USERNAME>@ssh.swc.ucl.ac.uk
```


. . . 

Log into HPC gateway node
```bash
ssh <USERNAME>@hpc-gw1
```

. . .

This node is fine for light work, but no intensive analyses

## Logging in

![](img/swc_hpc_access_flowchart.png){fig-align="center" width=100%}

. . .

::: {.callout-tip}
## More details

See our guide at [howto.neuroinformatics.dev](https://howto.neuroinformatics.dev/programming/SSH-SWC-cluster.html){preview-link="true"}
:::

## File systems  {.smaller}

* `/nfs/nhome/live/<USERNAME>` or `/nfs/ghome/live/<USERNAME>` 
  * "Home drive" (SWC/GCNU), also at `~/`
* `/nfs/winstor/<group>` - Old SWC research data storage (read-only soon)
* `/nfs/gatsbystor` - GCNU data storage
* `/ceph/<group>` - Current research data storage
* `/ceph/scratch` - Not backed up, for short-term storage
* `/ceph/apps` - HPC applications

. . .

::: {.callout-note}
You may only be able to "see" a drive if you navigate to it
:::

## 
Navigate to the scratch space
```bash
cd /ceph/scratch
```
. . .

Create a directory for yourself 
```bash
mkdir <USERNAME>
```


## HPC software
All nodes have the same software installed

* Ubuntu 20.04 LTS
* General linux utilities

## Modules
Preinstalled packages available for use, including:


:::: {.columns}

::: {.column width="40%"}
* BrainGlobe
* CUDA
* Julia
* Kilosort
:::

::: {.column width="60%"}
* mamba
* MATLAB
* miniconda
* SLEAP
:::

::::


## Using modules

List available modules
```bash
module avail
```
. . .

Load a module

```bash
module load SLEAP
```
. . .

Unload a module

```bash
module unload SLEAP
```
. . .

Load a specific version
```bash
module load SLEAP/2023-08-01
```
. . .

List loaded modules
```bash
module list
```


## SLURM
* Simple Linux Utility for Resource Management
* Job scheduler
* Allocates jobs to nodes
* Queues jobs if nodes are busy
* Users must explicitly request resources

## SLURM commands
View a summary of the available resources
```bash
sinfo
```

```
atyson@sgw2:~$ sinfo
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
cpu*         up   infinite     29  idle~ enc1-node[1,3-14],enc2-node[1-10,12-13],enc3-node[5-8]
cpu*         up   infinite      1  down* enc3-node3
cpu*         up   infinite      2    mix enc1-node2,enc2-node11
cpu*         up   infinite      5   idle enc3-node[1-2,4],gpu-380-[24-25]
gpu          up   infinite      9    mix gpu-350-[01,03-05],gpu-380-[10,13],gpu-sr670-[20-22]
gpu          up   infinite      9   idle gpu-350-02,gpu-380-[11-12,14-18],gpu-sr670-23
medium       up   12:00:00      4  idle~ enc3-node[5-8]
medium       up   12:00:00      1  down* enc3-node3
medium       up   12:00:00      1    mix gpu-380-10
medium       up   12:00:00     10   idle enc3-node[1-2,4],gpu-380-[11-12,14-18]
fast         up    3:00:00      1    mix gpu-380-10
fast         up    3:00:00      9   idle enc1-node16,gpu-380-[11-12,14-18],gpu-erlich01
```

##

View currently running jobs (from everyone)
```bash
squeue
```

```
atyson@sgw2:~$ squeue
JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
4036257       cpu     bash   imansd  R 13-01:10:01      1 enc1-node2
4050946       cpu      zsh apezzott  R 1-01:02:30      1 enc2-node11
3921466       cpu     bash   imansd  R 51-03:05:29      1 gpu-380-13
4037613       gpu     bash  pierreg  R 12-05:55:06      1 gpu-sr670-20
4051306       gpu ddpm-vae   jheald  R      15:49      1 gpu-350-01
4051294       gpu  jupyter    samoh  R    1:40:59      1 gpu-sr670-22
4047787       gpu     bash antonins  R 4-18:59:43      1 gpu-sr670-21
4051063_7       gpu    LRsem apezzott  R 1-00:08:32      1 gpu-350-05
4051063_8       gpu    LRsem apezzott  R 1-00:08:32      1 gpu-380-10
4051305       gpu     bash  kjensen  R      18:33      1 gpu-sr670-20
4051297       gpu     bash   slenzi  R    1:15:39      1 gpu-350-01
```

. . .

::: {.callout-tip}
## More details

See our guide at [howto.neuroinformatics.dev](https://howto.neuroinformatics.dev/programming/SLURM-arguments.html){preview-link="true"}
:::

## Partitions

## Interactive job
Start an interactive job (`bash -i`) in the cpu partition (`-p cpu`) in pseudoterminal mode (`--pty`).
```bash
srun -p cpu --pty bash -i
```
. . .

Always start a job (interactive or batch) before doing anything intensive to spare the gateway node.

## Run some "analysis"

Clone a test script
```bash
cd ~/
git clone https://github.com/neuroinformatics-unit/course-software-skills-hpc
```

. . .

Check out list of available modules
```bash
module avail
```
. . .

Load the miniconda module
```bash
module load miniconda
```

## 


Create conda environment
```bash
cd course-software-skills-hpc/demo
conda env create -f env.yml
```
. . .

Activate conda environment and run Python script
```bash
conda activate slurm_demo
python multiply.py 5 10 --jazzy
```
. . .

Stop interactive job
```bash
exit
```


## Batch jobs

Check out batch script:
```bash
cd course-software-skills-hpc/demo
cat batch_example.sh
```

```bash
#!/bin/bash

#SBATCH -p gpu # partition (queue)
#SBATCH -N 1   # number of nodes
#SBATCH --mem 2G # memory pool for all cores
#SBATCH -n 2 # number of cores
#SBATCH -t 0-0:10 # time (D-HH:MM)
#SBATCH -o slurm_output.out
#SBATCH -e slurm_error.err
#SBATCH --mail-type=ALL
#SBATCH --mail-user=adam.tyson@ucl.ac.uk

module load miniconda
conda activate slurm_demo

for i in {1..5}
do
  echo "Multiplying $i by 10"
  python multiply.py $i 10 --jazzy
done
```

## 

Run batch job:
```bash
sbatch batch_example.sh
```


## Array jobs


Check out array script:
```bash
cat array_example.sh
```

```bash
#!/bin/bash

#SBATCH -p gpu # partition (queue)
#SBATCH -N 1   # number of nodes
#SBATCH --mem 2G # memory pool for all cores
#SBATCH -n 2 # number of cores
#SBATCH -t 0-0:10 # time (D-HH:MM)
#SBATCH -o slurm_array_%A-%a.out
#SBATCH -e slurm_array_%A-%a.err
#SBATCH --mail-type=ALL
#SBATCH --mail-user=adam.tyson@ucl.ac.uk
#SBATCH --array=0-9%4

# Array job runs 10 separate jobs, but not more than four at a time.
# This is flexible and the array ID ($SLURM_ARRAY_TASK_ID) can be used in any way.

module load miniconda
conda activate slurm_demo

echo "Multiplying $SLURM_ARRAY_TASK_ID by 10"
python multiply.py $SLURM_ARRAY_TASK_ID 10 --jazzy
```

## 

Run array job:
```bash
sbatch array_example.sh
```


## Using GPUs

Start an interactive job with one GPU:
```bash
srun -p gpu --gres=gpu:1 --pty bash -i
```
. . .

Load TensorFlow & CUDA
```bash
module load tensorflow
module load cuda/11.8
```
. . .

Check GPU 
```bash
python
```

```python
import tensorflow as tf
tf.config.list_physical_devices('GPU')
```

## Useful commands
Cancel a job
```bash
scancel <JOBID>
```
. . .

Cancel all your jobs
```bash
scancel -u <USERNAME>
```

# Example: pose estimation with SLEAP {background-color="#03A062"}

## Modern behavioural analysis {.smaller}

:::: {.columns}

::: {.column width="70%"}
![](img/modern_behav_experiment_analysis.png){fig-align="center" height=400px}
:::

::: {.column width="30%"}
```{mermaid}
%%| file: img/diagrams/video_pipeline_pose.mmd
%%| fig-height: 400px
```
:::

::::

::: aside
Source: [Open-source tools for behavioral video analysis: Setup, methods, and best practices](https://elifesciences.org/articles/79305)
:::

## Pose estimation {.smaller}

![](img/pose_estimation_2D.png){fig-align="center"}

::: {.fragment}
- "easy" in humans - vast amounts of data
- "harder" in animals - less data, more variability
:::

::: aside
Source: [Quantifying behavior to understand the brain](https://www.nature.com/articles/s41593-020-00734-z)
:::

## Pose estimation software {.smaller}

:::: {.columns}

:::{.column width="50%"}
[DeepLabCut](http://www.mackenziemathislab.org/deeplabcut): *transfer learning*
:::

::: {.column width="50%"}
[SLEAP](https://sleap.ai/):*smaller networks*
:::
::::

![source: [sleap.ai](https://sleap.ai/)](img/sleap_movie.gif){fig-align="center" height="400px" style="text-align: center"}

::: aside
Many others: 
[LightningPose](https://github.com/danbider/lightning-pose),
[DeepPoseKit](https://github.com/jgraving/DeepPoseKit),
[Anipose](https://anipose.readthedocs.io/en/latest/),
...
:::

## Top-down pose estimation

![](img/pose_estimation_topdown.png)

## SLEAP workflow

![](img/diagrams/pose-estimation.svg){fig-align=center width=600}

::: {.fragment}
- Training and inference are GPU-intensive
- We can delegate to the HPC cluster's GPU nodes
:::

## Sample data
`/ceph/scratch/neuroinformatics-dropoff/SLEAP_HPC_test_data/course-hpc-2023`

- Mouse videos from [Loukia Katsouri](https://www.sainsburywellcome.org/web/people/loukia-katsouri)
- SLEAP project with:
  - labelled frames
  - trained models
  - prediction results

## Labeling data locally
![](img/sleap-labeling.png){fig-align=center}

## Exporting a training job package
![](img/sleap-training.png){fig-align=center}

::: aside
see also [SLEAP's guide for remote training](https://sleap.ai/guides/remote.html)
:::

## Copy the training job package {.smaller}

If you want to follow along with the next steps,
feel free to copy the unzipped training package to 
your scratch space:

```{.bash}
cp -r /ceph/scratch/neuroinformatics-dropoff/SLEAP_HPC_test_data/course-hpc-2023/labels.v001.slp.training_job /ceph/scratch/<USERNAME>/
```

## Training job package contents

```{.bash code-line-numbers=false}
# Copy of labelled frames
labels.v001.pkg.slp

# Model configuration files
centroid.json
centered_instance.json

# Bash scripts for running training and inference
train_script.sh
inference_script.sh

# Summary of all jobs
jobs.yaml
```

## Find  {.smaller}

```{.bash code-line-numbers="1-3|5-7|8-10|11-12"}
# Navigate to your scratch space
cd /ceph/scratch/<USERNAME>

# Check the contents of your folder
ls -l

# Go inside the exported training package
cd labels.v001.slp.training_job
ls -l

# View the contents of train-script.sh
cat train-script.sh
```

::: {.fragment}
```{.bash filename="train-script.sh"}
#!/bin/bash
sleap-train centroid.json labels.v001.pkg.slp
sleap-train centered_instance.json labels.v001.pkg.slp
```
:::

## Get SLURM to run the script {.smaller}

![](img/swc_hpc_access_flowchart.png){fig-align="center" width=100%}

::: {.panel-tabset}

### Batch
Main method for submitting jobs

- Prepare a batch script, e.g. `train_slurm.sh`
- Submit the job:
  ```{.bash code-line-numbers=false}
  sbatch train_slurm.sh
  ```
- Monitor job status: 
  ```{.bash code-line-numbers=false}
  squeue -u <SWC-USERNAME>
  ```

### Interactive
Suitable for debugging (immediate feedback)

- Start an interactive job with one GPU
  ```{.bash code-line-numbers=false}
  srun -p gpu --gres=gpu:1 --pty bash -i
  ```
- Execute commands one-by-one, e.g.:
  ```{.bash code-line-numbers=false}
  module load SLEAP
  cd <MY-TRAINING-DIRECTORY>
  bash train-script.sh

  # Stop the session
  exit
  ```
:::

## Further reading
* [SWC/GCNU Scientific Computing wiki](https://wiki.ucl.ac.uk/display/SSC/High+Performance+Computing)
* [SLURM documentation](https://slurm.schedmd.com/)